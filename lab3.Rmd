---
title: "Lab sessions"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(fpp3)
library(tidyverse)
```

## Lab 1

```{r lab1, options}
#------- Lab Session 1---------------
#1.1-Import data into R
ae_uk_original <- readr::read_csv("data/ae_uk.csv", 
                                  col_types = cols( 
                                    arrival_time=col_datetime(format = "%d/%m/%Y %H:%M"),
                                    gender=col_character(),
                                    type_injury=col_character()))

#1.2- check duplications and fix it
ae_uk_original %>% duplicated() %>% sum()#check duplicates
ae_wd <- ae_uk_original %>% dplyr::distinct()# remove duplicates and get a distinct tibble
# nrow(ae_uk_original)-nrow(ae_wd) check the number of duplication
#1.3- create tsibble
ae_tsb <- ae_wd %>% 
  as_tsibble(key = c(gender,type_injury), index = arrival_time, regular=FALSE)
# if you start working with a irregular index, you need to  use `regular=FALSE` in as_tsibble
# regularise an irregular index, create a new tsibble  
ae_hourly <- ae_tsb %>% group_by(gender,type_injury) %>% 
  index_by(arrival_1h = lubridate::floor_date(arrival_time, "1 hour")) %>% 
  summarise(n_attendance=n())

# 1.4. check implicit NA / gaps in time
has_gaps(ae_hourly)#check gaps
scan_gaps(ae_hourly)# show mw gaps
count_gaps(ae_hourly)# coun gaps
# if there is any gap, then fill it with zero
ae_hourly <- ae_tsb %>% group_by(gender, type_injury) %>% 
  index_by(arrival_1h = lubridate::floor_date(arrival_time, "1 hours")) %>% 
  summarise(n_attendance=n()) %>% fill_gaps(n_attendance=0L) %>% ungroup()
#you can use `index_by()` and `summarise()` to regularise index
# ae_hourly is a tsibble with regular space of 1 hour, you can change it to any interval,e.g. "2 hours","3 hours", etc or create any granular level from the hourly series such as daily, weekly , etc
# create a daily series to work with a single time series, in tsibble you can work many time series, go to lab session 12 for more information
ae_daily <- ae_hourly %>% 
  index_by(year_day=as_date(arrival_1h)) %>% 
  summarise(n_attendance=sum(n_attendance))
```

## Lab 2

```{r lab2, options}
# -------Lab Session 2------------
# time plot
ae_hourly %>% autoplot(n_attendance) # create a time plot of hourly data
ae_daily %>% autoplot(n_attendance) # create a time plot of daily data
# you can use filter_index to get a subset of data , try ?filter_index
# filter data to show only observations for february 2016
ae_daily %>% tsibble::filter_index("2016-02") %>% autoplot(n_attendance)

#you can plot monthly time series
ae_hourly %>% 
  index_by(year_month=yearmonth(arrival_1h)) %>% 
  summarise(n_attendance=sum(n_attendance)) %>% 
  autoplot(n_attendance) +
  labs(y = "attendances", x="Month",
       title = "Monthly A&E attendance",
       subtitle = "UK hospital")
# you can try year_quarter=yearquarter(arrival_1h)
```

## Lab 3

```{r lab3, options}
#---- Lab Session 3--------
# use seasonal and subseries plots to check wether series contain seasonality 

ae_hourly %>% gg_season(n_attendance) 
ae_hourly %>% gg_season(n_attendance,period = "week")# change period to period 

# craete subseries plot
ae_hourly %>% gg_subseries(n_attendance, period = "week")
ae_daily %>% gg_subseries(n_attendance, period = "week")

# How do you create a seasonal plot for the monthly and quartely series series
ae_hourly %>% index_by(year_month=yearmonth(arrival_1h)) %>%
  summarise(n_attendance=sum(n_attendance)) %>% 
  gg_season()# replace gg_season with gg_subseries()

#Question: is there any seasonality in the daily time series? what about hourly and monthly?
```

## Lab 4

```{r lab4, options}
# -------------------------Lab Session 4-------------------------

ae_daily %>% gg_lag(n_attendance, lags = c(1:14), geom = "point")# create lag plots for 14 lags, from 1 to 14
ae_daily %>% ACFlag_max = 14)# compute autocorrelation function
ae_daily %>% ACF(n_attendance, lag_max = 14) %>% -----# plot acf

ae_daily %>% gg_tsdisplay(n_attendance)# plot time plot, acf and season plot, check ?gg_tsdisplay

ae_daily %>% featu(n_attendance, ljung_box, dof=0)# use ljung box to test whether ACF is significant, if p-value is samll, << 0.05 then there is a significant autocorrelation 

```

## Lab 5 (optional)

```{r lab5, options}
# ------Lab Session 5 fittig model, specify and estimate in fable-----
# start with simple benchmark method: average, naive, snaive, drift
#how to specify a model MODELNAME(forecastvariable ~ term1+term2+...+term n), if there is no term ignore ~ term1+term2+...+termn
#---fitting: specify models and estimate parameters using model()---

ae_fit <- ae_daily %>%
  model(
    mean = MEAN(n_attendance),
    naive = NAIVE(n_attendance),
    snaive = SNAIVE(n_attendance)
  )
# ae_fit is called mable, it is model table, each row belongs to one time series and each column to a model
# to produce forecasts, we pass ae_fit, the mable objet into the forecast()
af_fc <- ae_fit %>% forecast(h="42 days")
# forecast needs the forecast horizon as an argument
af_fc %>% autoplot(filter_index(ae_daily,"2016"~.), level=95)
# we can plot generated forecasts using models, if you don;t want to plot prediction intervals, then level=NULL
ae_fit %>% augment() # what is the output of this line of code?
#you can extract fitted values and residuals for each model using augment()
# you can then use filter() to extract information for any model and select  .fitted or .resid
#you can replace .model== with any model you have use in tte model(), line 5-11
ae_fit %>% augment() %>% filter(.model=="mean")%>% select(.fitted)
ae_fit %>% augment() %>% filter(.model=="snaive")%>% select(.fitted) #%>% autoplot()
ae_fit %>% augment() %>% filter(.model=="snaive")%>% select(.resid) %>% autoplot()

#-----residual diagnostic------
ae_fit %>% select(mean) %>% gg_tsresiduals()# check residual for the fitted/trained model, do they look like white noise/random?
ae_fit %>% select(snaive) %>% gg_tsresiduals()# check residual for the fitted/trained model, do they look like white noise/random?
ae_fit %>% augment() %>% filter(.model=="mean") %>% 
   features(.resid, ljung_box, lag=14,dof=0)
# ljung_box to check residuals,  do you reject the null hypothesis? H0: Time series is random
```

## Lab 6: Time series cross validation

```{r lab6, options}
# -------------------------Lab Session 6 Time series cross validation-------------------------

# what is your forecast horizon?
f_horizon <- 42# forecast horizon

# split datainto test and the rest/train(initial training plus validation)
test <- ae_daily %>% slice((n()-(f_horizon-1)):n())# create test set equal to forecast horizon
train <- ae_daily %>% slice(1:(n()-f_horizon))# craete train set
nrow(ae_daily)==(nrow(test)+nrow(train))# check if split is correct, the result should be TRUE

# using time series cross validation(TSCV)/ rolling forecast
# why do we use TSCV?
# how do we do TSCV in R? 
#1. create different .id/time series using stretch_tsibble(), what is .init and .step?
#2. model/train each .id/time series and each model, 
#3. forecast for each .id and each model and given forecast horizon

#1. Create different time series using stretch_tsibble
train_tr <- train %>% # split data into folds with increasing size
  slice(1:(n()-42)) %>%
  stretch_tsibble(.init = 5*365, .step = 1)
# .init is the size of initial time series, .step is the increment step >=1
# what is the purpose of using slice(1:(n()-42))?
# how many time series/fold we create with this process? what is te new variable .id?
# 2. train models for each time series and each .id
ae_mode_tr <- train_tr %>%
  model(
    mean = MEAN(n_attendance),
    naive = NAIVE(n_attendance),
    snaive = SNAIVE(n_attendance)
  )
ae_mode_tr# observe mable
# Produce forecast for h=42 or 42 days for each .id and each model
ae_fc_tr <- ae_mode_tr %>% forecast(h=42)
ae_fc_tr#observe fable


# calculate forecast accuracy both point forecast and prediction interval using accuracy()
# accuracy() need both the forecast object/fable and all data in train
# what happens if you use train_tr, instead of train in the following code
fc_accuracy <- ae_fc_tr %>% accuracy(train, measures = list(
  point_accuracy_measures,
  interval_accuracy_measures
)) 
# you can specify which accuracy measure you want using measures = list()
# and give list of accuracy measure functions to compute 
# (such as point_accuracy_measures, interval_accuracy_measures, or distribution_accuracy_measures)
# if use  ae_fc_tr %>% accuracy(train), it calculates only point forecast

fc_accuracy %>% select(.model, RMSE, MAE, winkler)

# we can report the forecast accuracy using RMSE, MAE and winkler
# the accuracy measure is calculated for each .id/time series and then averaged across all foled(using mean())
# this will give us an average accuracy measure across all .id for each model

# you can plot forecasts
ae_model <- train %>% model(mean=MEAN(n_attendance))# specify and train MEAN method on all train data
ae_model %>% forecast(test) %>% # forecast and visualise it, you can provide the test set when using forecast() instead of h=42
  autoplot(filter_index(ae_daily,"2016"))

#-----residual diagnostic------

# results sghow that mean method has the lowes error among 4 methods we compared , we can check wether 
#this method captured adequate information/systematic patterns in the time series
# ae_fit %>% select(mean) %>% gg_tsresiduals()# 
ae_model %>% select(mean) %>% gg_tsresiduals()# check residual for the fitted/trained model, do they look like white noise/random?
ae_fit %>% filter(.model=="mean") %>% 
  augment() %>% ----(.resid, ljung_box, lag=14,dof=0)

#what do you conclude?

##-aditional exercise-------------
#sometime it might be useful to track the accuracy of the model across each period of the forecast horizon
#from h=1,2,3,4,...,42. to get that you need to follow the following steps

View(ae_fc_tr[1:43,])# in ae_fc_tr( a fable object) each .id is representing the forecast for each fold/ series creates
# if you want to get the forecast generated for each .id/series, each model and across all forecast horizon, then
#group_by() ,id and ,model first and then create a new variable called h using row_number() for forecast horizon
# check ?row_number
ae_fc <- ae_fc_tr %>% 
  group_by(.id,.model) %>% 
  mutate(h=row_number()) %>% ungroup()
View(ae_fc[1:43,])# view the first 43 rows of ae_fc toobserve h

# calculate forecast accuracy both point forecast and prediction interval using accuracy()
# accuracy() need both the forecast object/fable and all data in training test
# what happens if you use train_tr, instead of train in the following code
fc_accuracy <- ae_fc %>% accuracy(train,measures = list(
  point_accuracy_measures,
  interval_accuracy_measures
)) 
# you can specify which accuracy measure you want using measures = list()
# if use  ae_fc %>% accuracy(train), it calculates only point forecast
fc_accuracy %>% group_by(.model) %>% 
  summarise(RMSE=mean(RMSE),
            MAE=mean(MAE),
            winkler=mean(winkler))
# we can report the forecast accuracy using RMSE, MAE and winkler
# the accuracy measure is calculated for each fold/series create and then we average across all foled(using mean())
# this will give us an average accuracy measure across all folds
```

## Lab 7: Exponential Smoothing

```{r lab7, options}

# fitting ETS model
#E: Error, T:Trend, S: Seasonality
#N: No trend, no seasonality
#A  additive, Ad: additive dapmed
# M: multiplicative
# the function for ets models in fable is ETS
# if we don't provide arguments, then it is an automatic ETS
# check ?ETS
ets_model <- train %>%
  model(
    automatic_ets = ETS(n_attendance),# automatically tests all options and optimises AIC_c
    ses           = ETS(n_attendance ~ error("A")+trend("N")+season("N")),
    holt_winter   = ETS(n_attendance ~ error("A")+trend("A")+season("A")),
    ets_paramet   = ETS(n_attendance ~ error("A")+trend("Ad", alpha=0.1, phi=0.8)+season('A'))
    )

glance(ets_model)# it provides you with a summary of mable,
# ets_model %>% select(ets) %>% tidy()# provide 
ets_model %>% tidy()# provide 
ets_model %>% select(holt_winter) %>% report()# provide a nice output of one model with parameters estimated
ets_model %>% select(ses) %>% report()# provide a nice output of models with parameters estimated
# to forecast, we pass mable to forecast()
ets_model %>% select(ses) %>% components() %>% select(remainder) %>% autoplot()

fcst_ets <- ets_model %>%
  forecast(h = "42 days") 
# we cam visualise the forecast
fcst_ets%>%
  autoplot(ae_daily, level = NULL)







#--- 
#1. Create different time series using stretch_tsibble
# train_tr <- train %>% # split data into folds with increasing size
#   slice(1:(n()-42)) %>%
#   stretch_tsibble(.init = 5*365, .step = 1)
# # .init is the size of initial time series, .step is the increment step >=1
# # what is the purpose of using slice(1:(n()-42))?
# # how many time series/fold we create with this process? what is te new variable .id?
# 2. train models for each time series and each .id
ae_mode_tr <- train_tr %>%
  model(
    mean = MEAN(n_attendance),
    naive = NAIVE(n_attendance),
    snaive = SNAIVE(n_attendance),
    automatic_ets = ETS(n_attendance)
  )
ae_mode_tr# observe mable
# Produce forecast for h=42 or 42 days for each .id and each model
ae_fc_tr <- ae_mode_tr %>% forecast(h=42)
ae_fc_tr#observe fable


# calculate forecast accuracy both point forecast and prediction interval using accuracy()
# accuracy() need both the forecast object/fable and all data in train
# what happens if you use train_tr, instead of train in the following code
fc_accuracy <- ae_fc_tr %>% accuracy(train, measures = list(
  point_accuracy_measures,
  interval_accuracy_measures
)) 
# you can specify which accuracy measure you want using measures = list()
# and give list of accuracy measure functions to compute 
# (such as point_accuracy_measures, interval_accuracy_measures, or distribution_accuracy_measures)
# if use  ae_fc_tr %>% accuracy(train), it calculates only point forecast

fc_accuracy %>% select(.model, RMSE, MAE, winkler)

# we can report the forecast accuracy using RMSE, MAE and winkler
# the accuracy measure is calculated for each .id/time series and then averaged across all foled(using mean())
# this will give us an average accuracy measure across all .id for each model

# you can plot forecasts
ae_model <- train %>% model(mean=MEAN(n_attendance))# specify and train MEAN method on all train data
ae_model %>% forecast(test) %>% # forecast and visualise it, you can provide the test set when using forecast() instead of h=42
  autoplot(filter_index(ae_daily,"2016"))

#-----residual diagnostic------

# results sghow that mean method has the lowes error among 4 methods we compared , we can check wether 
#this method captured adequate information/systematic patterns in the time series
# ae_fit %>% select(mean) %>% gg_tsresiduals()# 
ae_model %>% select(mean) %>% gg_tsresiduals()# check residual for the fitted/trained model, do they look like white noise/random?
ae_fit %>% filter(.model=="mean") %>% 
  augment() %>% ----(.resid, ljung_box, lag=14,dof=0)







ets_tr <- ---- %>% ----(automatic_ets=----()) 
ets_fcst_tr <- ---- %>% ----(h=----)

fc_accuracy <- ----- %>% ----(----,measures = list(
  point_accuracy_measures,
  interval_accuracy_measures
)) 
fc_accuracy %>% select(.model,RMSE, MAE,winkler)

glance(----)# it provides you with a summary of mable,
ets_model %>% select(----) %>% tidy()# provide 
ets_model %>% select(----) %>% report()# provide a nice output of models with parameters estimated
# to forecast, we pass mable to forecast()

```
