---
title: "session2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(fpp3)
library(tidyverse)
```

## Lab 1

```{r lab1, options}
#------- Lab Session 1---------------
#1.1-Import data into R
ae_uk_original <- readr::read_csv("data/ae_uk.csv", 
                                  col_types = cols( 
                                    arrival_time=col_datetime(format = "%d/%m/%Y %H:%M"),
                                    gender=col_character(),
                                    type_injury=col_character()))

#1.2- check duplications and fix it
ae_uk_original %>% duplicated() %>% sum()#check duplicates
ae_wd <- ae_uk_original %>% dplyr::--------()# remove duplicates and get a distinct tibble
# nrow(ae_uk_original)-nrow(ae_wd) check the number of duplication
#1.3- create tsibble
ae_tsb <- ae_wd %>% 
  as_tsibble(key = c(gender,type_injury), index = --------, regular=-----)
# if you start working with a irregular index, you need to  use `regular=FALSE` in as_tsibble
# regularise an irregular index, create a new tsibble  
ae_hourly <- ae_tsb %>% group_by(gender,type_injury) %>% 
  index_by(arrival_1h = lubridate::---------(arrival_time, "1 hour")) %>% 
  summarise(n_attendance=n())

# 1.4. check implicit NA / gaps in time
has_gaps(ae_hourly)#check gaps
scan_gaps(ae_hourly)# show mw gaps
count_gaps(ae_hourly)# coun gaps
# if there is any gap, then fill it with zero
ae_hourly <- ae_tsb %>% group_by(gender, type_injury) %>% 
  ------(arrival_1h = lubridate::floor_date(arrival_time, "1 hours")) %>% 
  summarise(n_attendance=n()) %>% --------(n_attendance=0L) %>% ungroup()
#you can use `index_by()` and `summarise()` to regularise index
# ae_hourly is a tsibble with regular space of 1 hour, you can change it to any interval,e.g. "2 hours","3 hours", etc or create any granular level from the hourly series such as daily, weekly , etc
# create a daily series to work with a single time series, in tsibble you can work many time series, go to lab session 12 for more information
ae_daily <- ae_hourly %>% 
  index_by(year_day=-----(arrival_1h)) %>% 
  summarise(n_attendance=sum(n_attendance))
```

## Lab 2

```{r lab2, options}
# -------Lab Session 2------------
# time plot
ae_hourly %>% -----(n_attendance) # create a time plot of hourly data
ae_daily %>% autoplot(-----) # create a time plot of daily data
# you can use filter_index to get a subset of data , try ?filter_index
# filter data to show only observations for february 2016
ae_daily %>% tsibble::filter_index(----) %>% autoplot(n_attendance)

#you can plot monthly time series
ae_hourly %>% 
  index_by(year_month=-------(arrival_1h)) %>% 
  summarise(n_attendance=sum(n_attendance)) %>% 
  autoplot(n_attendance) +
  labs(y = "attendances", x="Month",
       title = "Monthly A&E attendance",
       subtitle = "UK hospital")
# you can try year_quarter=yearquarter(arrival_1h)
```

## Lab 3

```{r lab3, options}
#---- Lab Session 3--------
# use seasonal and subseries plots to check wether series contain seasonality 

ae_hourly %>% ------(n_attendance) 
ae_hourly %>% gg_season(n_attendance,period = -----)# change period to period 

# craete subseries plot
ae_hourly %>% -----(n_attendance)

# How do you create a seasonal plot for the monthly and quartely series series
ae_hourly %>% index_by(year_month=yearmonth(arrival_1h)) %>%
  summarise(n_attendance=sum(n_attendance)) %>% 
  gg_season()# replace gg_season with gg_subseries()

#Question: is there any seasonality in the daily time series? what about hourly and monthly?
```

## Lab 4

```{r lab4, options}
# -------------------------Lab Session 4-------------------------

ae_daily %>% gg_lag(n_attendance, lags = c(1:14), geom = "point")# create lag plots for 14 lags, from 1 to 14
ae_daily %>% ---(lag_max = 14)# compute autocorrelation fucntion
ae_daily %>% ACF(----, lag_max = 14) %>% -----# plot acf

ae_daily %>% gg_tsdisplay(-----)# plot time plot, acf and season plot, check ?gg_tsdisplay

ae_daily %>% -----(n_attendance, ljung_box, dof=0)# use ljung box to test wether ACF is significant, if p-value is amll, << 0.05 then there is a significant autocorrelation 

```

## Lab 5 (optional)

```{r lab5, options}
# ------Lab Session 5 fittig model, specify and estimate in fable-----
# start with simple benchmark method: average, naive, snaive, drift
#how to specify a model MODELNAME(forecastvariable ~ term1+term2+...+term n), if there is no term ignore ~ term1+term2+...+termn
#---fitting: specify models and estimate parameters using model()---

ae_fit <- ae_daily %>%
  model(
    mean = ----(n_attendance),
    naive = ----(n_attendance),
    snaive = ----(n_attendance)
  )
# ae_fit is called mable, it is model table, each row belongs to one time series and each column to a model
# to produce forecasts, we pass ae_fit, the mable objet into the forecast()
af_fc <- ae_fit %>% forecast(h="42 days")
# forecast needs the forecat horizon as an argument
af_fc %>% autoplot(filter_index(ae_daily,"2016"~.), level=NULL)
# we can plot generated forecasts using models, if you don;t want to plot prediction intervals, then level=NULL
ae_fit %>% augment() # what is the outpur of this line of code?
#you can extrat fitted values and residuals for each model usig augment()
# you can then use filter() to extract information for any model and select  .fitted or .resid
#you can replace .model== with anymodel you have use in tye model(), line 5-11
ae_fit %>% augment() %>% filter(.model=="snaive")%>% select(.fitted)
ae_fit %>% augment() %>% filter(.model=="mean")%>% select(.resid)

#-----residual diagnostic------
ae_fit %>% filter(.model=="mean") %>% gg_tsresiduals()# check residual for the fitted/trained model, do they look like white noise/random?
ae_fit %>% filter(.model=="mean") %>% 
  augment() %>% features(.resid, ljung_box, lag=14,dof=0)
# ljung_box to check residuals,  do you reject the null hypothesis? H0: Time series are random
```

## Lab 6: Time series cross validation

```{r lab6, options}
# -------------------------Lab Session 6 Time series cross validation-------------------------

# what is your forecast horizon?
f_horizon <- 42# forecast horizon

# split datainto test and the rest/train(initial training plus validation)
test <- ae_daily %>% slice((n()-(f_horizon-1)):n())# create test set equal to forecast horizon
train <- ae_daily %>% slice(1:(n()-f_horizon))# craete train set
nrow(ae_daily)==(nrow(test)+nrow(train))# check if split is correct, the result should be TRUE

# using time series cross validation(TSCV)/ rolling forecast
# why do we use TSCV?
# how do we do TSCV in R? 
#1. create different .id/time series using stretch_tsibble(), what is .init and .step?
#2. model/train each .id/time series and each model, 
#3. forecast for each .id and each model and given forecast horizon

#1. Create different timeseries using stretch_tsibble
train_tr <- train %>% # split data into folds with increasing size
  slice(1:(n()-42)) %>%
  ----(.init = 4*365, .step = ----)
# .init is the size of initial time series, .step is the increment step >=1
# what is the purpose of using slice(1:(n()-42))?
# how many time series/fold we create with this process? what is te new variable .id?
# 2. train models for each time series and each .id
ae_mode_tr <- train_tr %>%
  model(
    mean = ----(----),
    naive = ----(----),
    snaive = ----(----)
  )
ae_mode_tr# observe mable
# Produce forecast for h=42 or 42 days for each .id and each model
ae_fc_tr <- ae_mode_tr %>% ----(h=42)
ae_fc_tr#observe fable


# calculate forecast accuracy both point forecast and prediction interval using accuracy()
# accuracy() need both the forecast object/fable and all data in train
# what happens if you use train_tr, instead of train in the following code
fc_accuracy <- ---- %>% ----(train,measures = list(
  point_accuracy_measures,
  interval_accuracy_measures
)) 
# you can specify which accuracy measure you want using measures = list()
# if use  ae_fc %>% accuracy(train), it calculates only point forecast

fc_accuracy %>% select(.model,RMSE, MAE,winkler)

# we can report the forecast accuracy using RMSE, MAE and winkler
# the accuracy measure is calculated for each .id/time series and then averaged across all foled(using mean())
# this will give us an average accuracy measure across all .id for each model


# you can plot forecasts
ae_model <- train %>% model(MEAN(n_attendance))# specify and train MEAN method on all train data
ae_model %>% forecast(test) %>% # forecast and visualise it, you can provide the test set when using forecast() instead of h=42
  autoplot(filter_index(ae_daily,"2016"))

#-----residual diagnostic------

# results sghow that mean method has the lowes error among 4 methods we compared , we can check wether 
#this method captured adequate information/systematic patterns in the time series
ae_fit %>% filter(.model=="mean") %>% gg_tsresiduals()# check residual for the fitted/trained model, do they look like white noise/random?
ae_fit %>% filter(.model=="mean") %>% 
  augment() %>% features(.resid, ljung_box, lag=14,dof=0)

#what do you conclude?

##-aditional excercise-------------
#somnetime it might be useful to track the accuracy of the model across each period of the forecast horizon
#from h=1,2,3,4,...,42. to get that you need to follow the followign steps

View(ae_fc_tr[1:43,])# in ae_fc_tr( a fable object) each .id is representing the forecast for each fold/ series creates
# if you want to get the forecast generated for each .id/series, each model and across all forecast horizon, then
#group_by() ,id and ,model first and then create a new variable called h using row_number() for forecast horizon
# check ?row_number
ae_fc <- ae_fc_tr %>% 
  group_by(.id,.model) %>% 
  mutate(h=row_number()) %>% ungroup()
View(ae_fc[1:43,])# view the first 43 rows of ae_fc toobserve h

# calculate forecast accuracy both point forecast and prediction interval using accuracy()
# accuracy() need both the forecast object/fable and all data in training test
# what happens if you use train_tr, instead of train in the following code
fc_accuracy <- ae_fc %>% accuracy(train,measures = list(
  point_accuracy_measures,
  interval_accuracy_measures
)) 
# you can specify which accuracy measure you want using measures = list()
# if use  ae_fc %>% accuracy(train), it calculates only point forecast
fc_accuracy %>% group_by(.model) %>% 
  summarise(RMSE=mean(RMSE),
            MAE=mean(MAE),
            winkler=mean(winkler))
# we can report the forecast accuracy using RMSE, MAE and winkler
# the accuracy measure is calculated for each fold/series create and then we average across all foled(using mean())
# this will give us an average accuracy measure across all folds
```
